{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_offline_llm():\n",
    "    print(\"Loading offline language model...\")\n",
    "    \n",
    "    model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoGPTQForCausalLM.from_quantized(\n",
    "        model_id,\n",
    "        use_safetensors=True,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        use_triton=False,\n",
    "        quantize_config=None,\n",
    "        inject_fused_attention=False  # Disable for better compatibility\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from llama_cpp import Llama  # New import for GGUF models\n",
    "import re\n",
    "\n",
    "model_path = \"./depression_bert_model\"\n",
    "depression_tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "depression_model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "depression_model.to(device)\n",
    "\n",
    "def load_offline_llm():\n",
    "    print(\"Loading offline language model...\")\n",
    "    \n",
    "\n",
    "    model_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "    \n",
    "    # Initialize Llama model\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        n_ctx=2048,  # Context window size\n",
    "        n_threads=4,  # CPU threads\n",
    "        n_gpu_layers=33,  # Layers to offload to GPU\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Initialize the model\n",
    "llm_model = load_offline_llm()\n",
    "\n",
    "# Function to clean text input\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    return text.lower().strip()\n",
    "\n",
    "# Function to predict depression probability using the BERT model\n",
    "def predict_depression(text):\n",
    "    cleaned = clean_text(text)\n",
    "    inputs = depression_tokenizer(cleaned, return_tensors='pt', truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = depression_model(**inputs)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probs[0][1].item()  # Probability of positive class (depression)\n",
    "\n",
    "# Function to generate a response using the LLM\n",
    "def generate_response(prompt, history):\n",
    "    system_message = \"\"\"You are a mental health assistant. Respond with:\n",
    "- Empathetic validation\n",
    "- Follow-up questions when needed\n",
    "- Practical coping suggestions\n",
    "- Crisis resources when risk is detected\"\"\"\n",
    "    \n",
    "    full_prompt = f\"[INST] {system_message}\\n\\nChat History:\\n\" + \\\n",
    "                  \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in history]) + \\\n",
    "                  f\"\\n\\nUser: {prompt} [/INST]\"\n",
    "    \n",
    "    # Generate response\n",
    "    output = llm_model(\n",
    "        prompt=full_prompt,\n",
    "        max_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repeat_penalty=1.1,\n",
    "    )\n",
    "    \n",
    "    return output['choices'][0]['text'].strip()\n",
    "\n",
    "# Chat function to interact with the user\n",
    "def chat():\n",
    "    history = []\n",
    "    print(\"\\nðŸŒ± Mental Health Support Chatbot - Type 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            print(\"\\nTake care of yourself. You can return anytime.\")\n",
    "            break\n",
    "        \n",
    "        # Analyze input with depression model\n",
    "        depression_prob = predict_depression(user_input)\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Generate context-aware response based on depression probability\n",
    "        if depression_prob > 0.65:  # High risk\n",
    "            prompt = (\n",
    "                f\"User shows depression signs (probability: {depression_prob:.2f}). \"\n",
    "                \"Provide supportive response with crisis resources.\"\n",
    "            )\n",
    "        elif depression_prob > 0.4:  # Moderate risk\n",
    "            prompt = (\n",
    "                f\"User may be struggling (probability: {depression_prob:.2f}). \"\n",
    "                \"Ask a gentle follow-up question and suggest one coping strategy.\"\n",
    "            )\n",
    "        else:  # Low risk\n",
    "            prompt = \"Acknowledge the message and offer general support.\"\n",
    "        \n",
    "        # Generate and display response from LLM\n",
    "        response = generate_response(prompt, history)\n",
    "        print(f\"\\nðŸŒ¼ Bot: {response}\")\n",
    "        \n",
    "        history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        history = history[-6:]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
