{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mental health support model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c4147f1ea2482ab4f6691f28ef8a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openhermes-2.5-mistral-7b.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\anaconda3\\envs\\main\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\devan\\.cache\\huggingface\\hub\\models--TheBloke--OpenHermes-2.5-Mistral-7B-GGUF. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Download the GGUF model properly\n",
    "def download_model():\n",
    "    return hf_hub_download(\n",
    "        repo_id=\"TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\",\n",
    "        filename=\"openhermes-2.5-mistral-7b.Q4_K_M.gguf\",\n",
    "        revision=\"main\"\n",
    "    )\n",
    "\n",
    "def load_offline_llm():\n",
    "    print(\"Downloading mental health support model...\")\n",
    "    model_path = download_model()\n",
    "    \n",
    "    print(\"Initializing model...\")\n",
    "    return Llama(\n",
    "        model_path=model_path,\n",
    "        n_ctx=4096,\n",
    "        n_gpu_layers=35,\n",
    "        verbose=True  # Keep verbose for Jupyter compatibility\n",
    "    )\n",
    "\n",
    "# Load depression classification model\n",
    "model_path = \"./depression_bert_model\"\n",
    "depression_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "depression_model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "depression_model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# Initialize LLM\n",
    "try:\n",
    "    llm_model = load_offline_llm()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Try manual download from:\")\n",
    "    print(\"https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ¸ Mental Health Companion: I'm here to listen. Let's talk about how you're feeling.\n",
      "\n",
      "ðŸŒ± Companion: I'm so sorry to hear that you're feeling sad right now. It's really important that you know your emotions are valid, and it's okay to feel this way. Would you like to talk about what might be causing these feelings? Remember, there is always hope for better days ahead.\n",
      "(For your safety, I want to remind you that professional help is available. You're not alone in this.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ± Companion: I understand that sometimes words can feel insufficient in expressing the depth of emotions we experience. If you ever need someone to listen or if you'd like to share more about what's on your mind, please don't hesitate to reach out. Your feelings are valid and important, and I hope you find the support you need during this time.\n",
      "(For your safety, I want to remind you that professional help is available. You're not alone in this.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ± Companion: I am so sorry for the loss of your beloved dog. Losing a pet can be incredibly painful, as they become a part of our families and bring us so much joy. It's completely normal to feel overwhelmed with grief during this time. Is there anyone you can talk to or lean on for support? Remember that it's okay to take the time you need to process your feelings and mourn the loss of your furry friend.\n",
      "(For your safety, I want to remind you that professional help is available. You're not alone in this.)\n",
      "\n",
      "Remember: You're not alone. Reach out anytime.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced text cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Depression prediction with improved preprocessing\n",
    "def predict_depression(text):\n",
    "    cleaned = clean_text(text)\n",
    "    inputs = depression_tokenizer(\n",
    "        cleaned,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = depression_model(**inputs)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probs[0][1].item()\n",
    "\n",
    "# Optimized response generation with ChatML formatting\n",
    "def generate_response(user_input, history, risk_level):\n",
    "    # Dynamic system message based on risk level\n",
    "    system_messages = {\n",
    "        'high': \"Provide empathetic support, validate feelings, offer crisis resources immediately. \"\n",
    "                 \"Prioritize safety and professional help.\",\n",
    "        'moderate': \"Ask open-ended questions to understand their situation better. \"\n",
    "                    \"Suggest practical coping strategies. Show genuine concern.\",\n",
    "        'low': \"Offer general emotional support. Validate experiences and maintain supportive presence.\"\n",
    "    }\n",
    "    \n",
    "    chatml_prompt = \"<|im_start|>system\\n\"\n",
    "    chatml_prompt += system_messages[risk_level] + \"<|im_end|>\\n\"\n",
    "    \n",
    "    # Add conversation history\n",
    "    for msg in history[-4:]:  # Keep recent context\n",
    "        role = 'user' if msg['role'] == 'user' else 'assistant'\n",
    "        chatml_prompt += f\"<|im_start|>{role}\\n{msg['content']}<|im_end|>\\n\"\n",
    "    \n",
    "    # Add current message\n",
    "    chatml_prompt += f\"<|im_start|>user\\n{user_input}<|im_end|>\\n\"\n",
    "    chatml_prompt += \"<|im_start|>assistant\\n\"\n",
    "    \n",
    "    # Generate response with optimized parameters\n",
    "    response = llm_model(\n",
    "        prompt=chatml_prompt,\n",
    "        max_tokens=512,\n",
    "        temperature=0.65,\n",
    "        top_p=0.85,\n",
    "        repeat_penalty=1.15,\n",
    "        stop=[\"<|im_end|>\"]\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['text'].split('<|im_end|>')[0].strip()\n",
    "\n",
    "# Main chat interface with safety features\n",
    "def chat():\n",
    "    history = []\n",
    "    print(\"\\nðŸŒ¸ Mental Health Companion: I'm here to listen. Let's talk about how you're feeling.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nYou: \").strip()\n",
    "            if not user_input:\n",
    "                continue\n",
    "                \n",
    "            if user_input.lower() in {'quit', 'exit', 'bye'}:\n",
    "                print(\"\\nRemember: You're not alone. Reach out anytime.\")\n",
    "                break\n",
    "\n",
    "            # Analyze emotional state\n",
    "            depression_prob = predict_depression(user_input)\n",
    "            \n",
    "            # Determine risk level\n",
    "            if depression_prob > 0.7:\n",
    "                risk_level = 'high'\n",
    "                safety_note = \"\\n(For your safety, I want to remind you that professional help is available. \"\n",
    "                safety_note += \"You're not alone in this.)\"\n",
    "            elif depression_prob > 0.4:\n",
    "                risk_level = 'moderate'\n",
    "                safety_note = \"\"\n",
    "            else:\n",
    "                risk_level = 'low'\n",
    "                safety_note = \"\"\n",
    "\n",
    "            # Generate and display response\n",
    "            response = generate_response(user_input, history, risk_level)\n",
    "            print(f\"\\nðŸŒ± Companion: {response}{safety_note}\")\n",
    "            \n",
    "            # Update conversation history\n",
    "            history.extend([\n",
    "                {'role': 'user', 'content': user_input},\n",
    "                {'role': 'assistant', 'content': response}\n",
    "            ])\n",
    "            \n",
    "            # Maintain conversation context (last 3 exchanges)\n",
    "            history = history[-6:]\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTake care of yourself. Come back whenever you need support.\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
