{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading offline language model... This may take a moment.\n"
     ]
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for bitsandbytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:397\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Initialize the model and tokenizer\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m llm_model, llm_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_offline_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n\u001b[0;32m     46\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mload_offline_llm\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Load with optimizations for limited VRAM\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig\n\u001b[1;32m---> 25\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnf4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     33\u001b[0m     model_id,\n\u001b[0;32m     34\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Automatically optimize placement\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[1;32mc:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:433\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[1;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    431\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnused kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. These kwargs are not used in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 433\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:491\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.post_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_4bit_use_double_quant, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    493\u001b[0m ):\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    495\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    496\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:889\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m    883\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32mc:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:862\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[0;32m    857\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:399\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "\n",
    "# Load depression detection model (keep this part unchanged)\n",
    "model_path = \"./depression_bert_model\"\n",
    "depression_tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "depression_model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "depression_model.to(device)\n",
    "\n",
    "# Load offline LLM - Mistral 7B or similar model with 4-bit quantization\n",
    "def load_offline_llm():\n",
    "    print(\"Loading offline language model... This may take a moment.\")\n",
    "    \n",
    "    # Use 4-bit quantization to reduce VRAM requirements\n",
    "    model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"  # Quantized version\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Load with optimizations for limited VRAM\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",  # Automatically optimize placement\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=torch.float16,  # Use mixed precision\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "llm_model, llm_tokenizer = load_offline_llm()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def predict_depression(text):\n",
    "    cleaned = clean_text(text)\n",
    "    inputs = depression_tokenizer(cleaned, return_tensors='pt', truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = depression_model(**inputs)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probs[0][1].item()\n",
    "\n",
    "def generate_response(prompt, context=None):\n",
    "    # Create a system message to provide context for the model\n",
    "    system_message = \"You're a compassionate mental health assistant. Respond with empathy and validation.\"\n",
    "    \n",
    "    # Format the conversation history and prompt for the model\n",
    "    formatted_prompt = f\"<s>[INST] {system_message}\\n\\n\"\n",
    "    \n",
    "    if context:\n",
    "        # Add previous conversation turns (limited to last 3 for memory constraints)\n",
    "        for i, msg in enumerate(context[-3:]):\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            \n",
    "            if role == \"user\":\n",
    "                formatted_prompt += f\"User: {content}\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                formatted_prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    # Add the current prompt\n",
    "    formatted_prompt += f\"User: {prompt} [/INST]\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = llm_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate the response with conservative settings to manage memory\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=llm_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the output and extract just the assistant's response\n",
    "    full_output = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_output.split(prompt)[-1].strip()\n",
    "    \n",
    "    # Clean up the response (remove any system artifacts)\n",
    "    if \"Assistant:\" in response:\n",
    "        response = response.split(\"Assistant:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def chat():\n",
    "    history = []\n",
    "    print(\"\"\"\\n🌱 Mental Health Support Chatbot\n",
    "    I'm here to listen without judgment. You can:\n",
    "    - Share as much or little as you like\n",
    "    - Take breaks anytime with 'quit'\n",
    "    \"\"\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"\\nYou: \").strip()\n",
    "            if user_input.lower() in ['quit', 'exit']:\n",
    "                print(\"\\nTake care of yourself. You can return anytime.\")\n",
    "                break\n",
    "                \n",
    "            # Store the user message in history\n",
    "            history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            \n",
    "            # Analyze input\n",
    "            prob = predict_depression(user_input)\n",
    "            \n",
    "            if prob > 0.4:\n",
    "                # Generate question \n",
    "                question_prompt = f\"Based on this statement: '{user_input}', generate a gentle follow-up question\"\n",
    "                question = generate_response(question_prompt, history)\n",
    "                print(f\"\\n🌼 Bot: {question}\")\n",
    "                \n",
    "                # Store the bot's question in history\n",
    "                history.append({\"role\": \"assistant\", \"content\": question})\n",
    "                \n",
    "                # Get response\n",
    "                answer = input(\"\\nYou: \").strip()\n",
    "                if answer.lower() in ['quit', 'exit']:\n",
    "                    print(\"\\nThank you for sharing. Be kind to yourself today.\")\n",
    "                    break\n",
    "                \n",
    "                # Store the user's follow-up in history\n",
    "                history.append({\"role\": \"user\", \"content\": answer})\n",
    "                \n",
    "                # Generate advice\n",
    "                advice_prompt = f\"User shared: '{user_input}' then '{answer}'. Provide 2-3 supportive suggestions.\"\n",
    "                advice = generate_response(advice_prompt, history)\n",
    "                print(f\"\\n🌱 Bot: {advice}\")\n",
    "                \n",
    "                # Store the bot's advice in history\n",
    "                history.append({\"role\": \"assistant\", \"content\": advice})\n",
    "                \n",
    "                if prob > 0.7:\n",
    "                    print(\"\\nIf you need immediate support:\")\n",
    "                    print(\"- National Suicide Prevention Lifeline: 1-800-273-TALK\")\n",
    "                    print(\"- Crisis Text Line: Text HOME to 741741\")\n",
    "                \n",
    "            else:\n",
    "                # Generate acknowledgment\n",
    "                response = generate_response(f\"Acknowledge this feeling: {user_input}\", history)\n",
    "                print(f\"\\n🌸 Bot: {response}\")\n",
    "                \n",
    "                # Store the bot's response in history\n",
    "                history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "            # Continue prompt\n",
    "            cont = input(\"\\nPress Enter to continue or type 'quit': \").strip()\n",
    "            if cont.lower() in ['quit', 'exit']:\n",
    "                print(\"\\nRemember: Progress isn't linear. Be proud you reached out!\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nIt's okay to pause. Come back when you're ready.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bitsandbytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(bitsandbytes\u001b[38;5;241m.\u001b[39m__version__)  \u001b[38;5;66;03m# Should output 0.41.1 or higher\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'"
     ]
    }
   ],
   "source": [
    "import bitsandbytes\n",
    "print(bitsandbytes.__version__)  # Should output 0.41.1 or higher\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
