{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "\n",
    "# Load depression detection model (keep this part unchanged)\n",
    "model_path = \"./depression_bert_model\"\n",
    "depression_tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "depression_model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "depression_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "llm_model, llm_tokenizer = load_offline_llm()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def predict_depression(text):\n",
    "    cleaned = clean_text(text)\n",
    "    inputs = depression_tokenizer(cleaned, return_tensors='pt', truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = depression_model(**inputs)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probs[0][1].item()\n",
    "\n",
    "def generate_response(prompt, history):\n",
    "    system_message = \"\"\"You are a mental health assistant. Respond with:\n",
    "    - Empathetic validation\n",
    "    - Follow-up questions when needed\n",
    "    - Practical coping suggestions\n",
    "    - Crisis resources when risk is detected\"\"\"\n",
    "    \n",
    "    # Format full history using Mistral's template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        *history,\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = llm_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = llm_tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "def chat():\n",
    "    history = []\n",
    "    print(\"\\nðŸŒ± Mental Health Support Chatbot - Type 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            break\n",
    "            \n",
    "        # 1. Analyze with depression model\n",
    "        depression_prob = predict_depression(user_input)\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # 2. Generate context-aware response\n",
    "        if depression_prob > 0.65:  # High risk\n",
    "            prompt = (\n",
    "                f\"User shows depression signs (probability: {depression_prob:.2f}). \"\n",
    "                \"Provide supportive response with crisis resources.\"\n",
    "            )\n",
    "        elif depression_prob > 0.4:  # Moderate risk\n",
    "            prompt = (\n",
    "                f\"User may be struggling (probability: {depression_prob:.2f}). \"\n",
    "                \"Ask a gentle follow-up question and suggest one coping strategy.\"\n",
    "            )\n",
    "        else:  # Low risk\n",
    "            prompt = \"Acknowledge the message and offer general support.\"\n",
    "        \n",
    "        # Generate and display response\n",
    "        response = generate_response(prompt, history)\n",
    "        print(f\"\\nðŸŒ¼ Bot: {response}\")\n",
    "        history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        # 3. Store conversation history (optional: save to file)\n",
    "        # Keep only last 6 messages for context\n",
    "        history = history[-6:]\n",
    "\n",
    "def chat():\n",
    "    history = []\n",
    "    print(\"\"\"\\nðŸŒ± Mental Health Support Chatbot\n",
    "    I'm here to listen without judgment. You can:\n",
    "    - Share as much or little as you like\n",
    "    - Take breaks anytime with 'quit'\n",
    "    \"\"\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"\\nYou: \").strip()\n",
    "            if user_input.lower() in ['quit', 'exit']:\n",
    "                print(\"\\nTake care of yourself. You can return anytime.\")\n",
    "                break\n",
    "                \n",
    "            # Store the user message in history\n",
    "            history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            \n",
    "            # Analyze input\n",
    "            prob = predict_depression(user_input)\n",
    "            \n",
    "            if prob > 0.4:\n",
    "                # Generate question \n",
    "                question_prompt = f\"Based on this statement: '{user_input}', generate a gentle follow-up question\"\n",
    "                question = generate_response(question_prompt, history)\n",
    "                print(f\"\\nðŸŒ¼ Bot: {question}\")\n",
    "                \n",
    "                # Store the bot's question in history\n",
    "                history.append({\"role\": \"assistant\", \"content\": question})\n",
    "                \n",
    "                # Get response\n",
    "                answer = input(\"\\nYou: \").strip()\n",
    "                if answer.lower() in ['quit', 'exit']:\n",
    "                    print(\"\\nThank you for sharing. Be kind to yourself today.\")\n",
    "                    break\n",
    "                \n",
    "                # Store the user's follow-up in history\n",
    "                history.append({\"role\": \"user\", \"content\": answer})\n",
    "                \n",
    "                # Generate advice\n",
    "                advice_prompt = f\"User shared: '{user_input}' then '{answer}'. Provide 2-3 supportive suggestions.\"\n",
    "                advice = generate_response(advice_prompt, history)\n",
    "                print(f\"\\nðŸŒ± Bot: {advice}\")\n",
    "                \n",
    "                # Store the bot's advice in history\n",
    "                history.append({\"role\": \"assistant\", \"content\": advice})\n",
    "                \n",
    "                if prob > 0.7:\n",
    "                    print(\"\\nIf you need immediate support:\")\n",
    "                    print(\"- National Suicide Prevention Lifeline: 1-800-273-TALK\")\n",
    "                    print(\"- Crisis Text Line: Text HOME to 741741\")\n",
    "                \n",
    "            else:\n",
    "                # Generate acknowledgment\n",
    "                response = generate_response(f\"Acknowledge this feeling: {user_input}\", history)\n",
    "                print(f\"\\nðŸŒ¸ Bot: {response}\")\n",
    "                \n",
    "                # Store the bot's response in history\n",
    "                history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "            # Continue prompt\n",
    "            cont = input(\"\\nPress Enter to continue or type 'quit': \").strip()\n",
    "            if cont.lower() in ['quit', 'exit']:\n",
    "                print(\"\\nRemember: Progress isn't linear. Be proud you reached out!\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nIt's okay to pause. Come back when you're ready.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes\n",
    "print(bitsandbytes.__version__)  # Should output 0.41.1 or higher\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
